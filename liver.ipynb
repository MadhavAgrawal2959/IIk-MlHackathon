{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 270. MiB for an array with shape (35389440,) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m image_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessed_data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m label_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessed_data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 29\u001b[0m X_train, Y_train \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Normalize images (0-1) and convert labels to binary (0 or 1)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m X_train \u001b[38;5;241m=\u001b[39m X_train \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n",
      "Cell \u001b[1;32mIn[24], line 18\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(image_folder, label_folder)\u001b[0m\n\u001b[0;32m     15\u001b[0m label_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(label_folder) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_file, lbl_file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(image_files, label_files):\n\u001b[1;32m---> 18\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     lbl \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(label_folder, lbl_file))\n\u001b[0;32m     20\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(img)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\lib\\npyio.py:456\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[0;32m    454\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 456\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\lib\\format.py:809\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[0;32m    808\u001b[0m         \u001b[38;5;66;03m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[1;32m--> 809\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    811\u001b[0m         \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[0;32m    822\u001b[0m         array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mndarray(count, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 270. MiB for an array with shape (35389440,) and data type float64"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import pandas as pd\n",
    "\n",
    "# ============ 1️⃣ LOAD AND PREPROCESS DATA ============ #\n",
    "def load_data(image_folder, label_folder):\n",
    "    \"\"\"\n",
    "    Load preprocessed images and labels from .npy files.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    image_files = sorted([f for f in os.listdir(image_folder) if f.startswith(\"image\")])\n",
    "    label_files = sorted([f for f in os.listdir(label_folder) if f.startswith(\"label\")])\n",
    "\n",
    "    for img_file, lbl_file in zip(image_files, label_files):\n",
    "        img = np.load(os.path.join(image_folder, img_file))\n",
    "        lbl = np.load(os.path.join(label_folder, lbl_file))\n",
    "        images.append(img)\n",
    "        labels.append(lbl)\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Paths to preprocessed data\n",
    "image_folder = \"preprocessed_data\"\n",
    "label_folder = \"preprocessed_data\"\n",
    "\n",
    "X_train, Y_train = load_data(image_folder, label_folder)\n",
    "\n",
    "# Normalize images (0-1) and convert labels to binary (0 or 1)\n",
    "X_train = X_train / 255.0\n",
    "Y_train = (Y_train > 0).astype(np.uint8)  # Convert to binary mask\n",
    "\n",
    "# Expand dimensions for model compatibility\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "Y_train = np.expand_dims(Y_train, axis=-1)\n",
    "\n",
    "print(f\"✅ Data Loaded: {X_train.shape[0]} images, Shape: {X_train.shape[1:]}\")\n",
    "\n",
    "# ============ 2️⃣ LIGHTWEIGHT U-NET MODEL ============ #\n",
    "def build_unet(input_shape=(256, 256, 1)):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder (downsampling)\n",
    "    c1 = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(p1)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(p2)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    # Bottleneck\n",
    "    b = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p3)\n",
    "\n",
    "    # Decoder (upsampling)\n",
    "    u1 = layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same')(b)\n",
    "    u1 = layers.Concatenate()([u1, c3])\n",
    "    c4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u1)\n",
    "\n",
    "    u2 = layers.Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same')(c4)\n",
    "    u2 = layers.Concatenate()([u2, c2])\n",
    "    c5 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(u2)\n",
    "\n",
    "    u3 = layers.Conv2DTranspose(16, (3, 3), strides=(2, 2), padding='same')(c5)\n",
    "    u3 = layers.Concatenate()([u3, c1])\n",
    "    c6 = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(u3)\n",
    "\n",
    "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(c6)\n",
    "\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "model = build_unet()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# ============ 3️⃣ TRAIN THE MODEL ============ #\n",
    "EPOCHS = 10  # Adjust if needed\n",
    "BATCH_SIZE = 4  # Reduce batch size for memory optimization\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.1)\n",
    "\n",
    "# ============ 4️⃣ PREDICT ON TEST DATA ============ #\n",
    "def predict_and_save(model, test_folder, output_folder):\n",
    "    test_images = sorted([f for f in os.listdir(test_folder) if f.startswith(\"image\")])\n",
    "    \n",
    "    for test_file in test_images:\n",
    "        img = np.load(os.path.join(test_folder, test_file)) / 255.0\n",
    "        img = np.expand_dims(img, axis=(0, -1))  # Add batch and channel dimensions\n",
    "        \n",
    "        pred_mask = model.predict(img)[0]  # Get prediction\n",
    "        pred_mask = (pred_mask > 0.5).astype(np.uint8)  # Threshold to binary\n",
    "        \n",
    "        np.save(os.path.join(output_folder, test_file.replace(\"image\", \"pred_mask\")), pred_mask)\n",
    "        print(f\"✅ Saved: {test_file}\")\n",
    "\n",
    "test_folder = \"preprocessed_test_data\"  # Update this if test data is in a different folder\n",
    "output_folder = \"output_predictions\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "predict_and_save(model, test_folder, output_folder)\n",
    "\n",
    "# ============ 5️⃣ GENERATE SUBMISSION FILE ============ #\n",
    "def rle_encode(mask):\n",
    "    \"\"\"\n",
    "    Convert a binary mask to Run-Length Encoding (RLE).\n",
    "    \"\"\"\n",
    "    pixels = mask.flatten(order='F')\n",
    "    pixels = np.concatenate([[0], pixels, [0]])  # Add sentinel values\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return \" \".join(str(x) for x in runs) if runs.size else \"1 0\"\n",
    "\n",
    "submission = []\n",
    "for file in sorted(os.listdir(output_folder)):\n",
    "    if file.startswith(\"pred_mask\"):\n",
    "        slice_id = file.replace(\"pred_mask_\", \"\").replace(\".npy\", \"\").replace(\"-\", \"_\")\n",
    "        mask = np.load(os.path.join(output_folder, file))\n",
    "        rle_mask = rle_encode(mask)\n",
    "        submission.append([slice_id, rle_mask])\n",
    "\n",
    "df_submission = pd.DataFrame(submission, columns=['id', 'rle'])\n",
    "df_submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"✅ Submission file created: submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
